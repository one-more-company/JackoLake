{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68156eb5-9224-4464-8a2a-c15908ca1d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# SparkSession - Delta\n",
       "| Spark Session        | http://localhost:4040        |\n",
       "|----------------------|------------------------------|\n",
       "| Spark Master         | http://localhost:5050        |\n",
       "| Spark Worker A       | http://localhost:5051        |\n",
       "| Spark Worker B       | http://localhost:5052        | \n",
       "| Hive Metastore       | thrift://hive-metastore:9083 |\n",
       "\n",
       "| MinIO Object Storage | http://localhost:9090 |\n",
       "|----------------------|-----------------------|\n",
       "| user                 | `admin`               |\n",
       "| password             | `password`            |\n",
       "\n",
       "- Use `%trino` or `%%trino` to run queries using Trino\n",
       "- Use `%sparksql` or `%%sparksql` to run queries using SparkSQL\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a20dc4-0584-4a17-ae20-38fd3d9d0f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-19f823f5-c6ef-4ca0-a0b3-757cbe9d8c0c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-storage;3.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.0.0/delta-spark_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.12;3.0.0!delta-spark_2.12.jar (477ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/3.0.0/delta-storage-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;3.0.0!delta-storage.jar (22ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.9.3/antlr4-runtime-4.9.3.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.9.3!antlr4-runtime.jar (50ms)\n",
      ":: resolution report :: resolve 2121ms :: artifacts dl 559ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-19f823f5-c6ef-4ca0-a0b3-757cbe9d8c0c\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (5230kB/15ms)\n",
      "25/03/26 20:13:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/26 20:13:31 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e544d9f0c140:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkSession-Delta</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff039cd5750>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 20:21:46 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/03/26 20:21:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkSession-Delta\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.0.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aa6cacb-3c2e-497d-b8e5-fba6262337f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import  functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, BooleanType, ByteType\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75bf1d17-34b4-43f5-9bc4-900e6a27e1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf25ab326974caaa9c5b28927e6f967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SparkSchemaWidget(nodes=(Node(close_icon='angle-down', close_icon_style='danger', icon='project-diagram', icon…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a class=\"external\" href=\"http://209ab534ddfc:4041\" target=\"_blank\" >Open Spark UI ⭐ SparkSession-Delta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><table border='1' class='mathjax_ignore'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sparksql drop table conformed.date_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25606d82-5c6b-42e6-a537-1969887ec822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 5519f7d5-8b6f-4eab-b369-9da7a6ca9d57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS conformed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "404d8800-aaf4-494e-b6e7-3c2221fef447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 07:51:37 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`conformed`.`date_dimension` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the Delta table\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS conformed.date_dimension(\n",
    "    Date date,\n",
    "    Date_ID long,\n",
    "    Day integer ,\n",
    "    Day_Name string ,\n",
    "    Is_Weekend boolean ,\n",
    "    Week integer ,\n",
    "    Month integer ,\n",
    "    Month_Name string ,\n",
    "    Quarter integer ,\n",
    "    Year integer ,\n",
    "    Is_Holiday boolean)\n",
    "    USING DELTA\n",
    "    LOCATION 's3a://test/date_dimension'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0171ac79-b088-4601-aa98-e9468d6724e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|      Date|Date_ID|\n",
      "+----------+-------+\n",
      "|2024-01-01|   8766|\n",
      "|2024-01-02|   8767|\n",
      "|2024-01-03|   8768|\n",
      "|2024-01-04|   8769|\n",
      "|2024-01-05|   8770|\n",
      "|2024-01-06|   8771|\n",
      "|2024-01-07|   8772|\n",
      "|2024-01-08|   8773|\n",
      "|2024-01-09|   8774|\n",
      "|2024-01-10|   8775|\n",
      "|2024-01-11|   8776|\n",
      "|2024-01-12|   8777|\n",
      "|2024-01-13|   8778|\n",
      "|2024-01-14|   8779|\n",
      "|2024-01-15|   8780|\n",
      "|2024-01-16|   8781|\n",
      "|2024-01-17|   8782|\n",
      "|2024-01-18|   8783|\n",
      "|2024-01-19|   8784|\n",
      "|2024-01-20|   8785|\n",
      "+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DateDimension\").getOrCreate()\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = \"2027-12-31\"\n",
    "\n",
    "# Create a DataFrame with a sequence of dates\n",
    "date_df = spark.sql(f\"\"\"\n",
    "    SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) AS date_range\n",
    "\"\"\").selectExpr(\"explode(date_range) AS Date\")\n",
    "\n",
    "# Debug: Check the Date column values and type\n",
    "\n",
    "# Add Date_ID column and debug\n",
    "date_df_with_id = date_df.withColumn(\"Date_ID\", ((F.unix_timestamp(\"Date\") - lit(946684800)) / lit(86400)).cast(\"long\"))\n",
    "date_df_with_id.show()\n",
    "\n",
    "# Add date attributes\n",
    "date_dimension = (date_df_with_id\n",
    "    .withColumn(\"Day\", F.dayofmonth(\"Date\")) \n",
    "    .withColumn(\"Day_Name\", F.date_format(\"Date\", \"EEEE\")) \n",
    "    .withColumn(\"Is_Weekend\", when(F.dayofweek(\"Date\").isin(1, 7), lit(True)).otherwise(lit(False))) \n",
    "    .withColumn(\"Week\", F.weekofyear(\"Date\")) \n",
    "    .withColumn(\"Month\", F.month(\"Date\")) \n",
    "    .withColumn(\"Month_Name\", F.date_format(\"Date\", \"MMMM\")) \n",
    "    .withColumn(\"Quarter\", F.quarter(\"Date\")) \n",
    "    .withColumn(\"Year\", F.year(\"Date\")) \n",
    "    .withColumn(\"Is_Holiday\", lit(False)))  # Placeholder for holiday logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc9a1de0-33f5-44a0-b41d-140d49053abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table saved to s3a://test/date_dimension\n"
     ]
    }
   ],
   "source": [
    "s3_path = \"s3a://test/date_dimension\"\n",
    "\n",
    "# Write Date Dimension to S3 as a Delta table\n",
    "date_dimension.write.format(\"delta\").partitionBy(\"Year\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(s3_path)\n",
    "\n",
    "print(f\"Delta table saved to {s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e37ae-6c95-4718-bd18-70f1cb93e078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767d17c-5871-4eba-b00c-99fe16989ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Delta)",
   "language": "python",
   "name": "pyspark_delta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

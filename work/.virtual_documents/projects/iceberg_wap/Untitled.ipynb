get_ipython().run_line_magic("SparkSession", "")


from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName("SparkSession-Delta") \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.0.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .enableHiveSupport() \
    .getOrCreate()

spark


from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql import  functions as F
from pyspark.sql import SparkSession
import json
from pyspark.sql import SparkSession
from pyspark.sql.types import ArrayType, StructType, StructField, StringType, BooleanType, ByteType
import requests


get_ipython().run_line_magic("sparksql", " drop table conformed.date_dimension")


spark.sql("CREATE DATABASE IF NOT EXISTS conformed")


# Register the Delta table
spark.sql(f"""
    CREATE TABLE IF NOT EXISTS conformed.date_dimension(
    Date date,
    Date_ID long,
    Day integer ,
    Day_Name string ,
    Is_Weekend boolean ,
    Week integer ,
    Month integer ,
    Month_Name string ,
    Quarter integer ,
    Year integer ,
    Is_Holiday boolean)
    USING DELTA
    LOCATION 's3a://test/date_dimension'
""")


from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import when, col, lit

# Initialize Spark session
spark = SparkSession.builder.appName("DateDimension").getOrCreate()

# Define start and end dates
start_date = "2024-01-01"
end_date = "2027-12-31"

# Create a DataFrame with a sequence of dates
date_df = spark.sql(f"""
    SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) AS date_range
""").selectExpr("explode(date_range) AS Date")

# Debug: Check the Date column values and type

# Add Date_ID column and debug
date_df_with_id = date_df.withColumn("Date_ID", ((F.unix_timestamp("Date") - lit(946684800)) / lit(86400)).cast("long"))
date_df_with_id.show()

# Add date attributes
date_dimension = (date_df_with_id
    .withColumn("Day", F.dayofmonth("Date")) 
    .withColumn("Day_Name", F.date_format("Date", "EEEE")) 
    .withColumn("Is_Weekend", when(F.dayofweek("Date").isin(1, 7), lit(True)).otherwise(lit(False))) 
    .withColumn("Week", F.weekofyear("Date")) 
    .withColumn("Month", F.month("Date")) 
    .withColumn("Month_Name", F.date_format("Date", "MMMM")) 
    .withColumn("Quarter", F.quarter("Date")) 
    .withColumn("Year", F.year("Date")) 
    .withColumn("Is_Holiday", lit(False)))  # Placeholder for holiday logic



s3_path = "s3a://test/date_dimension"

# Write Date Dimension to S3 as a Delta table
date_dimension.write.format("delta").partitionBy("Year").mode("overwrite").option("overwriteSchema", "true").save(s3_path)

print(f"Delta table saved to {s3_path}")






